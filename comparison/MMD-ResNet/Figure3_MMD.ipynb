{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Scanpy 1.7.2, on 2021-06-18 14:06.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from metrics import compute_entropy, silhouette_coeff_ASW\n",
    "import torch\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sc.settings.verbosity = 3\n",
    "sc.settings.set_figure_params(dpi=100)\n",
    "sc.settings.set_figure_params(dpi_save=300)\n",
    "sc.logging.print_version_and_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "from math import log\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def shannon_entropy (x, b_vec, N_b):\n",
    "    \n",
    "    tabled_values = b_vec[x > 0].value_counts()/ len(b_vec[x >0]) #class 'pandas.core.series.Series'\n",
    "\n",
    "    tabled_val = tabled_values.tolist() \n",
    "    \n",
    "    entropy = 0.0\n",
    "    for element in tabled_val:\n",
    "        if element != 0:\n",
    "            entropy += element * log(element)\n",
    "            \n",
    "    entropy /= log(N_b)\n",
    "\n",
    "    return(-entropy) #the entropy formula is the -sum, this is why we include the minus sign\n",
    "\n",
    "def compute_entropy(adata, output_entropy=None, batch_key='batch', celltype_key='celltype'):\n",
    "    print(\"Calculating entropy ...\")\n",
    "    kwargs = {}\n",
    "    #batch vector(batch id of each cell)\n",
    "    kwargs['batch_vector'] = adata.obs[batch_key]\n",
    "    #modify index of batch vector so it coincides with matrix's index\n",
    "    kwargs['batch_vector'].index = range(0,len(kwargs['batch_vector']))\n",
    "    #number of batches\n",
    "    kwargs['N_batches'] = len(adata.obs[batch_key].astype('category').cat.categories)\n",
    "\n",
    "    #cell_type vector( betch id of each cell)\n",
    "    kwargs['cell_type_vector'] = adata.obs[celltype_key]\n",
    "    #modify index of cell_type vector so it coincides with matrix's index\n",
    "    kwargs['cell_type_vector'].index = range(0,len(kwargs['cell_type_vector']))\n",
    "    #number of cell_types\n",
    "    kwargs['N_cell_types'] = len(adata.obs[celltype_key].astype('category').cat.categories)    \n",
    "\n",
    "    try:\n",
    "        knn_graph = adata.uns['neighbors']\n",
    "        print('use exist neighbors')\n",
    "    except KeyError:\n",
    "        #compute neighbors\n",
    "        print('compute neighbors')\n",
    "        sc.tl.pca(adata)\n",
    "        sc.pp.neighbors(adata)\n",
    "\n",
    "    #knn graph\n",
    "    knn_graph = adata.uns['neighbors']['connectivities']\n",
    "    #transforming csr_matrix to dataframe\n",
    "    df = pd.DataFrame(knn_graph.toarray())\n",
    "    \n",
    "    #apply function\n",
    "    batch_entropy = df.apply(shannon_entropy, axis=0, args=(kwargs['batch_vector'],kwargs['N_batches']))\n",
    "    cell_type_entropy = df.apply(shannon_entropy, axis=0, args=(kwargs['cell_type_vector'] ,kwargs['N_cell_types']))\n",
    "    print(\"Entropy calculated!\")\n",
    "    \n",
    "    results = {'batch': batch_entropy, \"cell_type\":cell_type_entropy}\n",
    "    results = pd.concat(results, axis = 1, keys = ['batch', 'cell_type'])\n",
    "    \n",
    "    if output_entropy:\n",
    "        results.to_csv(output_entropy, header = True, index = False)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def silhouette_coeff_ASW(adata, method_use='raw',save_dir='', save_fn='', percent_extract=0.8, batch_key='batch', celltype_key='celltype'):\n",
    "    asw_fscore = []\n",
    "    asw_bn = []\n",
    "    asw_bn_sub = []\n",
    "    asw_ctn = []\n",
    "    iters = []\n",
    "    for i in range(20):\n",
    "        iters.append('iteration_'+str(i+1))\n",
    "        rand_cidx = np.random.choice(adata.obs_names, size=int(len(adata.obs_names) * percent_extract), replace=False)\n",
    "        adata_ext = adata[rand_cidx,:]\n",
    "        asw_batch = silhouette_score(adata_ext.obsm['X_pca'], adata_ext.obs[batch_key])\n",
    "        asw_celltype = silhouette_score(adata_ext.obsm['X_pca'], adata_ext.obs[celltype_key])\n",
    "        min_val = -1\n",
    "        max_val = 1\n",
    "        asw_batch_norm = (asw_batch - min_val) / (max_val - min_val)\n",
    "        asw_celltype_norm = (asw_celltype - min_val) / (max_val - min_val)\n",
    "        \n",
    "        fscoreASW = (2 * (1 - asw_batch_norm)*(asw_celltype_norm))/(1 - asw_batch_norm + asw_celltype_norm)\n",
    "        asw_fscore.append(fscoreASW)\n",
    "        asw_bn.append(asw_batch_norm)\n",
    "        asw_bn_sub.append(1-asw_batch_norm)\n",
    "        asw_ctn.append(asw_celltype_norm)\n",
    "    \n",
    "\n",
    "    df = pd.DataFrame({'asw_batch_norm':asw_bn, 'asw_batch_norm_sub': asw_bn_sub,\n",
    "                       'asw_celltype_norm': asw_ctn, 'fscore':asw_fscore,\n",
    "                       'method_use':np.repeat(method_use, len(asw_fscore))})\n",
    "#     df.to_csv(save_dir + save_fn + '.csv')\n",
    "#     print('Save output of pca in: ',save_dir)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#首先将每个批次数据切分开，总共有2个批次\n",
    "adata = sc.read_h5ad('/media/bgi/zoubin/single_cell/batch_correction/data/dataset5.h5ad')\n",
    "adata0 = adata[adata.obs.batch=='0']\n",
    "adata1 = adata[adata.obs.batch=='1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizing by total count per cell\n",
      "Trying to set attribute `.obs` of view, copying.\n",
      "    finished (0:00:02): normalized adata.X and added    'n_counts', counts per cell before normalization (adata.obs)\n",
      "computing PCA\n",
      "    with n_comps=50\n",
      "    finished (0:00:21)\n",
      "normalizing by total count per cell\n",
      "Trying to set attribute `.obs` of view, copying.\n",
      "    finished (0:00:01): normalized adata.X and added    'n_counts', counts per cell before normalization (adata.obs)\n",
      "computing PCA\n",
      "    with n_comps=50\n",
      "    finished (0:00:24)\n"
     ]
    }
   ],
   "source": [
    "#adata0 processed\n",
    "sc.pp.normalize_per_cell(adata0, counts_per_cell_after=1e4)\n",
    "sc.pp.log1p(adata0)\n",
    "sc.pp.scale(adata0)\n",
    "sc.tl.pca(adata0, svd_solver='arpack')\n",
    "\n",
    "#adata1 processed\n",
    "sc.pp.normalize_per_cell(adata1, counts_per_cell_after=1e4)\n",
    "sc.pp.log1p(adata1)\n",
    "sc.pp.scale(adata1)\n",
    "sc.tl.pca(adata1, svd_solver='arpack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成MMD算法输入的文件，2个只包含表达矩阵的csv\n",
    "data0 = pd.DataFrame(adata0.obsm['X_pca'])\n",
    "data1 = pd.DataFrame(adata1.obsm['X_pca'])\n",
    "\n",
    "data0.to_csv('Figure3_dataset5_0_time.csv',index=None,header=None)\n",
    "data1.to_csv('Figure3_dataset5_1_time.csv',index=None,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.253, counter: 0\n",
      "Learning rate = 0.0000100\n",
      "Epoch 2, loss: 0.252, counter: 0\n",
      "Learning rate = 0.0000100\n",
      "Epoch 3, loss: 0.252, counter: 0\n",
      "Learning rate = 0.0000100\n",
      "Epoch 4, loss: 0.252, counter: 1\n",
      "Learning rate = 0.0000100\n",
      "Epoch 5, loss: 0.253, counter: 2\n",
      "Learning rate = 0.0000100\n",
      "Epoch 6, loss: 0.252, counter: 3\n",
      "Learning rate = 0.0000100\n",
      "Epoch 7, loss: 0.253, counter: 4\n",
      "Learning rate = 0.0000100\n",
      "Epoch 8, loss: 0.254, counter: 5\n",
      "Learning rate = 0.0000100\n",
      "Finished training\n",
      "Figure(800x800)\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 327, in <module>\n",
      "    main()\n",
      "  File \"train.py\", line 306, in main\n",
      "    name2='sample2')\n",
      "  File \"/media/bgi/zhouruilong/deepMNN/Reproduction of result/MMD-ResNet/scatterHist.py\", line 67, in scatterHist\n",
      "    fig.savefig(plots_dir+'/'+title+'.eps' ,format='eps')\n",
      "  File \"/media/bgi/zhouruilong/anaconda3/envs/pytorch1.6/lib/python3.6/site-packages/matplotlib/figure.py\", line 2203, in savefig\n",
      "    self.canvas.print_figure(fname, **kwargs)\n",
      "  File \"/media/bgi/zhouruilong/anaconda3/envs/pytorch1.6/lib/python3.6/site-packages/matplotlib/backend_bases.py\", line 2126, in print_figure\n",
      "    **kwargs)\n",
      "  File \"/media/bgi/zhouruilong/anaconda3/envs/pytorch1.6/lib/python3.6/site-packages/matplotlib/backends/backend_ps.py\", line 831, in print_eps\n",
      "    return self._print_ps(outfile, 'eps', *args, **kwargs)\n",
      "  File \"/media/bgi/zhouruilong/anaconda3/envs/pytorch1.6/lib/python3.6/site-packages/matplotlib/backends/backend_ps.py\", line 851, in _print_ps\n",
      "    orientation, papertype, **kwargs)\n",
      "  File \"/media/bgi/zhouruilong/anaconda3/envs/pytorch1.6/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py\", line 358, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/media/bgi/zhouruilong/anaconda3/envs/pytorch1.6/lib/python3.6/site-packages/matplotlib/backends/backend_ps.py\", line 1076, in _print_figure\n",
      "    with open(outfile, 'w', encoding='latin-1') as fh:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/media/bgi/zhouruilong/batchEffectRemoval2020-master/Figure3_result/Data before calibration.eps'\n",
      "time used: 62.968278884887695\n"
     ]
    }
   ],
   "source": [
    "#将train.py中的file1，file2的读取地址改成上面两个csv,运行MMD\n",
    "#生成的2个经过批次矫正的结果   sample1.csv.npy，   calibrated_sample2.csv.npy\n",
    "time_s = time.time()\n",
    "!python train.py\n",
    "print('time used:', time.time()-time_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0 = np.load(\"sample1.csv.npy\")\n",
    "data1 = np.load(\"calibrated_sample2.csv.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将矫正后数据保存会adata\n",
    "adata = sc.read_h5ad('/media/bgi/zoubin/single_cell/batch_correction/data/dataset5.h5ad')\n",
    "adata0 = adata[adata.obs.batch=='0']\n",
    "adata1 = adata[adata.obs.batch=='1']\n",
    "adata0.obsm['X_pca'] = data0\n",
    "adata1.obsm['X_pca'] = data1\n",
    "adata_MMD = adata0.concatenate(adata1,index_unique=None,batch_categories=['10X 3\\'','10X 5\\''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot result\n",
    "sc.pp.neighbors(adata_MMD)\n",
    "sc.tl.umap(adata_MMD)\n",
    "\n",
    "sc.pl.umap(adata_MMD,color=['batch'],save='_figure3_MMD_batch.pdf')\n",
    "sc.pl.umap(adata_MMD,color=['celltype'],save='_figure3_MMD_celltype.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asw_MMD = silhouette_coeff_ASW(adata_test)\n",
    "entropy_MMD = compute_entropy(adata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asw_MMD.to_csv('asw_MMD.csv',index=0)\n",
    "entropy_MMD.to_csv('entropy_MMD.csv',index=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1.6",
   "language": "python",
   "name": "pytorch1.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
